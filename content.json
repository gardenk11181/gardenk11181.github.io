{"meta":{"title":"Garden","subtitle":"","description":"","author":"Jeongwon Kim","url":"https://gardenk11181.github.io","root":"/"},"pages":[],"posts":[{"title":"VAE - (3) Results","slug":"vae-03","date":"2021-08-30T01:24:24.000Z","updated":"2021-08-30T02:43:53.635Z","comments":true,"path":"2021/08/30/vae-03/","link":"","permalink":"https://gardenk11181.github.io/2021/08/30/vae-03/","excerpt":"","text":"Previous Article: VAE - (2) Architecture 총 두가지 데이터셋에 대하여 학습을 진행하였다. 동일하게 100 epoch만큼 훈련시켰으며, 사용한 Adam Optimizer의 parameter는 beta1이 0.9, beta2가 0.999이다. batch 크기는 100, learning rate은 0.001로 설정하였다. MNISTMNIST데이터에 대한 설명은 앞선 글에서 했으니 바로 결과를 분석해보자. 위쪽 4X8의 이미지들은 input image이며, 이를 encoder와 decoder, 그리고 sigmoid를 적용했을 때의 reconstructed image는 아래쪽 4X8 이미지에 해당한다. 비록 MNIST가 간단한 데이터지만 굉장히 정교하게 reconstruct함을 알 수 있다. 위 그림의 8X8 이미지들은 각각 gaussian분포에서 랜덤하게 뽑은 8차원 벡터를 decoder 및 sigmoid를 적용시킨 이미지이다. 비록 숫자와는 거리가 먼 그림들도 생성되었지만, 8~90%가 손글씨 이미지처럼 보임을 알 수 있다. MNIST데이터는 28X28의 단일채널 이미지이므로 784차원에 해당하는데 이 VAE모델을 이용한다면 단 8차원만으로 다양한 MNIST데이터를 생성할 수 있다는 것을 의미한다. YaleB FaceYaleB Face데이터는 총 28명의 사람들의 광원의 각도, 표정에 따른 얼굴이 제공되는 데이터이다. 마찬가지로 reconstruction과 generation결과를 살펴보면 아래와 같다. 마찬가지로 위쪽은 input image, 아래쪽은 reconstructed image에 해당한다. 한눈에 알 수 있듯이 얼굴 데이터는 상당히 복잡하며 차원 역시 64X64로 높은 데이터임에도 본래 이미지를 잘 reconstruct함을 알 수 있다. 한 사람의 얼굴만 있는 것이 아니라 28명의 얼굴이 있음에도 모든 사람의 얼굴을 잘 reconstruct한다는 것은 학습된 모델이 주어진 28명의 얼굴형태나 특징을 잘 함축하여 가지고 있다고 볼 수 있다. 반면에 generated image는 그리 정교하지 못하다. 추측이지만, MNIST에 비해 훨씬 복잡한 데이터임에도 decoder의 architecture나 epoch수가 그대로인 것이 주된 원인으로 보인다. 추후에 conditional model도 다뤄볼 예정인데, generation 결과가 해당 모델에 대해서는 어떻게 개선되는지 살펴보자. Summary요약하자면, VAE의 성능은 준수한 것으로 보인다. DCGAN보다 안정적으로 수렴했으며, batch-normalization layer없이도 잘 학습되었다. 특히 reconstruction의 성능은 데이터의 크기와 관계없이 상당히 좋았다. 다만, 조금 복잡한 데이터의 경우에 generation이 정교하게 되지 못한다는 점이 이 모델의 유일한 단점으로 보이는데, 더 좋은 architecture나 충분한 수의 epoch가 제공된다면 해결되어질 것으로 예상된다.","categories":[{"name":"AI","slug":"AI","permalink":"https://gardenk11181.github.io/categories/AI/"},{"name":"VAE","slug":"AI/VAE","permalink":"https://gardenk11181.github.io/categories/AI/VAE/"}],"tags":[{"name":"ai","slug":"ai","permalink":"https://gardenk11181.github.io/tags/ai/"},{"name":"deep learning","slug":"deep-learning","permalink":"https://gardenk11181.github.io/tags/deep-learning/"},{"name":"vae","slug":"vae","permalink":"https://gardenk11181.github.io/tags/vae/"}]},{"title":"VAE - (2) Architecture","slug":"vae-02","date":"2021-08-09T03:28:19.000Z","updated":"2021-08-30T02:14:09.548Z","comments":true,"path":"2021/08/09/vae-02/","link":"","permalink":"https://gardenk11181.github.io/2021/08/09/vae-02/","excerpt":"","text":"Previous Article: VAE - (1) Auto-Encoding Variational Bayes 다른 Auto Encoder와 마찬가지로 VAE도 여러 가지 Architecture로 구현할 수 있을 것이다. 다만, Fully Connected Layer로만 구성된 모델은 Image 데이터의 크기가 커질수록 성능에 한계가 있기 때문에 Convolutional Layer로 VAE를 구성하는 것이 일반적이다. I. Encoder 먼저, Encoder의 구조는 위와 같다. 주어진 이미지가 네 개의 Convolutional layer를 거쳐서 두 개의 8차원 벡터로 mapping 된다. (그림에는 하나의 8차원 벡터만 표현되었다.) 각각은 평균과 분산의 log 값에 해당한다. 이후에 각 평균과 분산을 parameter로 가지는 8개의 정규분포에서 sampling 하는 reparametrication 과정을 통해 우리가 원하는 8차원의 latent vector를 얻는다. II. Decoder Decoder의 구조는 위와 같이 Encoder의 역순이라고 보면 된다. 점차 height과 width를 늘려 나가는 과정이므로 네 개의 Convolutional Transpose layer를 사용한다. 이후에는 Convolutional layer를 통해 3-channel의 image를 reconstruction 한다. VAE의 Architecture가 앞서 다룬 DCGAN이나 다른 Auto Encoder 들과 다른 점은 Batch Normalization layer가 없다는 것이다. tensorflow tutorial에 따르면 reparametrication 과정에 따른 무작위성이 이미 존재하기에 여기에 Batch Normalization layer가 추가될 경우 오히려 모델의 정확성이 떨어진다고 언급되어 있다. 실제로 Batch Normalization layer를 넣어서 학습시켜보면 결과물이 뿌옇게 나오는 것을 알 수 있었다. 따라서 VAE에서는 Convolutional layer와 ReLU만을 사용한다. 또한, input image와 reconstructed image의 오차인 loss값을 계산할 때는 decoder로부터 나온 결과에 추가적으로 sigmoid function을 적용한 값을 사용한다. 즉, activation layer가 decoder에는 없고 별도로 sigmoid를 적용하는 것이다. 우리가 결과를 확인할 때 살펴보는 reconstructed image 역시 sigmoid가 적용된 image에 해당한다. 다음 글에서는 pytorch를 통해 학습된 VAE의 결과물들을 살펴보고 이를 해석해보도록 하자. Next Article: VAE - (3) Results","categories":[{"name":"AI","slug":"AI","permalink":"https://gardenk11181.github.io/categories/AI/"},{"name":"VAE","slug":"AI/VAE","permalink":"https://gardenk11181.github.io/categories/AI/VAE/"}],"tags":[{"name":"ai","slug":"ai","permalink":"https://gardenk11181.github.io/tags/ai/"},{"name":"deep learning","slug":"deep-learning","permalink":"https://gardenk11181.github.io/tags/deep-learning/"},{"name":"vae","slug":"vae","permalink":"https://gardenk11181.github.io/tags/vae/"}]},{"title":"VAE - (1) Auto-Encoding Variational Bayes","slug":"vae-01","date":"2021-08-06T05:48:08.000Z","updated":"2021-08-09T07:01:27.453Z","comments":true,"path":"2021/08/06/vae-01/","link":"","permalink":"https://gardenk11181.github.io/2021/08/06/vae-01/","excerpt":"","text":"어쩌다 보니 인턴을 시작한 지 한 달이 지났다. 그동안 VAE, WAE 등의 Auto Encoder 모델을 공부하였는데, 이번 글에서는 내가 주로 맡은 VAE에 대해 적어보고자 한다. Auto Encoder는 주어진 이미지를 낮은 차원의 latent vector로 mapping 하는 Encoder와, 이 latent vector를 다시 주어진 이미지와 비슷한 이미지로 mapping 하는 Decoder로 이루어진 Neural Network이다. 모델을 잘 학습시킨다면 낮은 차원의 vector만을 가지고도 본래의 고용량 이미지를 만들 수도 있고, 임의의 vector를 가지고 새로운 이미지를 생성할 수도 있다. 전자와 후자를 각각 reconstruction과 generation라고 한다. 예를 들자면, AI에서 자주 등장하는 28X28 MNIST 데이터를 학습시킨다고 가정해보자. 첫째로, 주어진 784차원의 이미지를 8차원의 vector로 mapping 할 수 있으며(encoding), 이 vector만을 가지고 본래의 이미지와 비슷한 이미지를 만들어낼 수 있다(decoding). 둘째로는, 임의의 8차원 vector로 새로운 손글씨 숫자 이미지를 생성할 수 있는 것이다. 이러한 Auto Encoder의 종류는 대개 loss function을 어떻게 정의하냐에 따라 나뉜다. VAE의 경우에는 decoder에서 해당 데이터가 나올 확률의 log 값의 lower bound와, encoder에서의 posterior 분포와 실제 posterior 분포의 KL-Divergence 값의 차로 정의된다. 이렇게 정의하게 된 배경과 이론에 대해서는 VAE 마지막 글에서 다뤄보도록 하겠다. 다음 글에서는 VAE에 사용되는 Architecture에 대하여 알아보자. Next Article: VAE - (2) Architecture","categories":[{"name":"AI","slug":"AI","permalink":"https://gardenk11181.github.io/categories/AI/"},{"name":"VAE","slug":"AI/VAE","permalink":"https://gardenk11181.github.io/categories/AI/VAE/"}],"tags":[{"name":"ai","slug":"ai","permalink":"https://gardenk11181.github.io/tags/ai/"},{"name":"deep learning","slug":"deep-learning","permalink":"https://gardenk11181.github.io/tags/deep-learning/"},{"name":"vae","slug":"vae","permalink":"https://gardenk11181.github.io/tags/vae/"}]},{"title":"DCGAN - (5) Discriminator 구현 및 학습 결과","slug":"dcgan-05","date":"2021-07-06T02:50:33.000Z","updated":"2021-07-09T04:08:04.314Z","comments":true,"path":"2021/07/06/dcgan-05/","link":"","permalink":"https://gardenk11181.github.io/2021/07/06/dcgan-05/","excerpt":"","text":"Previous Article: DCGAN - (4) Generator 구현 tensorflow tutorial에서 제시하는 Discriminator Architecture는 좀 더 간단하며, 총 세가지의 layer로 이루어져 있다. Generator와 다르게 크기를 줄여나가야 하므로 일반적인 convolutional layer를 사용한다. tensorflow에서의 code는 아래와 같다. 123456789101112131415def make_discriminator_model(): model = tf.keras.Sequential() model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding=&#x27;same&#x27;, input_shape=[28, 28, 1])) model.add(layers.LeakyReLU()) model.add(layers.Dropout(0.3)) model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding=&#x27;same&#x27;)) model.add(layers.LeakyReLU()) model.add(layers.Dropout(0.3)) model.add(layers.Flatten()) model.add(layers.Dense(1)) return model 위 코드를 pytorch에서 구현해보자. 첫번째 layer에서는 28X28X1 image를 14X14X64 feature maps로 만든다. 이 때, parameter는 in_channel: 1, out_channel: 64, kernel_size: 5, stride: 2, padding: 2을 사용한다. 두번째 layer에서는 14X14X64 feature maps를 다시 7X7X128 feature maps로 만든다. parameter는 마찬가지로 in_channel: 64, out_channel: 128, kernel_size: 5, stride: 2, padding: 2을 사용한다. 마지막 layer에서는 7X7X128 feature maps를 scala로 만든다. 이 때, tensorflow에서는 Flatten 이후에 Dense(1)로 지정하면 바로 scala로 mapping해주지만, pytorch에서는 Flatten했을 때의 node 개수를 Linear(#node,1)의 형태로 지정해줘야 한다. 구현한 pytorch 코드는 아래와 같다. 12345678910111213141516171819class Discriminator(nn.Module): def __init__(self, ngpu): super(Discriminator, self).__init__() self.ngpu = ngpu self.main = nn.Sequential( nn.Conv2d(1,64,5,2,2,bias=False), nn.LeakyReLU(0.3,True), nn.Dropout2d(0.3), nn.Conv2d(64,128,5,2,2,bias=False), nn.LeakyReLU(0.3,True), nn.Dropout2d(0.3), nn.Flatten(), nn.Linear(7*7*128,1), nn.Sigmoid() ) def forward(self, input): return self.main(input) 이렇게 설정을 마친 후에 학습을 진행하게 되면 다음과 같은 images가 생성됨을 확인할 수 있었다. 다소 Blur된 부분도 보이고, 부정확한 숫자도 보이지만 숫자스러운(?) 모양이 나타났다는 것에 의의를 두도록 하겠다. 이 GAN모델은 첫번째 글에서 언급한대로 두 분포 가 같음이 보장되어야 하기에 안정성 측면에서 이슈가 있으며 위와 같이 부정확한 결과가 도출되기도 하는 것이다. GAN에 대한 공부는 여기서 마치고, 보다 발전된 모델을 알아보도록 하자.","categories":[{"name":"AI","slug":"AI","permalink":"https://gardenk11181.github.io/categories/AI/"},{"name":"DCGAN","slug":"AI/DCGAN","permalink":"https://gardenk11181.github.io/categories/AI/DCGAN/"}],"tags":[{"name":"ai","slug":"ai","permalink":"https://gardenk11181.github.io/tags/ai/"},{"name":"deep learning","slug":"deep-learning","permalink":"https://gardenk11181.github.io/tags/deep-learning/"},{"name":"gan","slug":"gan","permalink":"https://gardenk11181.github.io/tags/gan/"}]},{"title":"DCGAN - (4) Generator 구현","slug":"dcgan-04","date":"2021-07-02T01:00:00.000Z","updated":"2021-07-07T07:07:26.526Z","comments":true,"path":"2021/07/02/dcgan-04/","link":"","permalink":"https://gardenk11181.github.io/2021/07/02/dcgan-04/","excerpt":"","text":"Previous Article: DCGAN - (3) Architecture pytorch tutorial에서 제시한 celebA데이터의 경우에는 크기가 제법 되기에 1차적인 목표는 MNIST 데이터를 통해 구현해보는 것이다. MNIST data를 pytorch로 가져오는 방법은 여기, MNIST data에 대한 DCGAN model은 여기에 나와있다. Architecture 구성을 제외한 모든 과정은 pytorch tutorial과 동일하게 진행하면 되기에 두 모델을 설정하는 부분만 살펴보고, 나머지는 깃허브 링크로 남겨두겠다. 위 링크에서 다룬 tensorflow tutorial에서의 Generator network는 아래와 같다. 1234567891011121314151617181920212223def make_generator_model(): model = tf.keras.Sequential() model.add(layers.Dense(7*7*256, use_bias=False, input_shape=(100,))) model.add(layers.BatchNormalization()) model.add(layers.LeakyReLU()) model.add(layers.Reshape((7, 7, 256))) assert model.output_shape == (None, 7, 7, 256) # 주목: 배치사이즈로 None이 주어집니다. model.add(layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding=&#x27;same&#x27;, use_bias=False)) assert model.output_shape == (None, 7, 7, 128) model.add(layers.BatchNormalization()) model.add(layers.LeakyReLU()) model.add(layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding=&#x27;same&#x27;, use_bias=False)) assert model.output_shape == (None, 14, 14, 64) model.add(layers.BatchNormalization()) model.add(layers.LeakyReLU()) model.add(layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), padding=&#x27;same&#x27;, use_bias=False, activation=&#x27;tanh&#x27;)) assert model.output_shape == (None, 28, 28, 1) return model 총 네가지의 layer로 이루어져 있음을 알 수 있다. 첫번째 layer에서는 100X1 latent vector로 7X7X256의 feature maps을 만든다. 위에서는 dense layer를 통해 총 7X7X256개의 node를 만들고 reshape을 했지만, 우리는 pytorch tutorial에서 사용한 방법과 동일하게 convolutional layer를 이용할 것이다. 이 때, in_channel: 100, out_channel: 256, kernel size: 7, stride: 1의 convolutional transpose layer를 이용한다. pytorch는 tensorflow처럼 output shape의 크기에 맞게 알아서 padding을 넣어주지 않기 때문에 직접 padding 값을 계산해야 한다. 이는 convolutional layer의 원리에 대한 이해가 필요하다. (조만간 자세하게 다루는 글을 작성할 것이다.) 두번째 layer에서는 in_channel: 256, out_channel: 128, kernel size: 5, stride: 1, padding: 2의 convolutional transpose layer를 이용하여 7X7X128 feature maps을 만든다. 세번째 layer에서는 위 모델과 조금 다르게 진행할 수밖에 없었다. pytorch에서 Conv2DTranspose layer의 원리 상, stride가 2이고 kernel size가 홀수일 때 width와 height이 짝수인 feature map을 만들지 못한다. 이는 tensorflow와 달리 pytorch에서는 asymmetric padding을 주지 못하기 때문이다. 따라서 kernel size 5는 유지하되, output_padding을 1만큼 줌으로써 절충하였다. 즉, in_channel: 128, out_channel: 64, kernel size: 5, stride: 2, padding: 2, output_padding: 1의 convolutional transpose layer를 이용하여 14X14X64 feature maps을 만들었다. 마지막 layer에서도 마찬가지 이유로 in_channel: 64, out_channel: 1, kernel size: 5, stride: 2, padding: 2, output_padding: 1의 convolutional transpose layer를 이용하여 28X28X1 feature maps(fake image)를 만들었다. 최종 코드는 아래와 같다. 12345678910111213141516171819202122class Generator(nn.Module): def __init__(self, ngpu): super(Generator, self).__init__() self.ngpu = ngpu self.main = nn.Sequential( nn.ConvTranspose2d(100,256,7,1,0,bias=False), nn.BatchNorm2d(256), nn.LeakyReLU(True), nn.ConvTranspose2d(256,128,5,1,2,bias=False), nn.BatchNorm2d(128), nn.LeakyReLU(True), nn.ConvTranspose2d(128,64,4,2,1,bias=False), nn.BatchNorm2d(64), nn.LeakyReLU(True), nn.ConvTranspose2d(64,1,4,2,1,bias=False), nn.Tanh() ) def forward(self, input): return self.main(input) 어쩌다 보니 글이 길어졌기에, Discriminator network와 학습결과는 다음 글에서 이어나가도록 하겠다. Next Article: DCGAN - (5) Descriminator 구현 및 학습 결과","categories":[{"name":"AI","slug":"AI","permalink":"https://gardenk11181.github.io/categories/AI/"},{"name":"DCGAN","slug":"AI/DCGAN","permalink":"https://gardenk11181.github.io/categories/AI/DCGAN/"}],"tags":[{"name":"ai","slug":"ai","permalink":"https://gardenk11181.github.io/tags/ai/"},{"name":"deep learning","slug":"deep-learning","permalink":"https://gardenk11181.github.io/tags/deep-learning/"},{"name":"gan","slug":"gan","permalink":"https://gardenk11181.github.io/tags/gan/"}]},{"title":"DCGAN - (3) Training","slug":"dcgan-03","date":"2021-06-30T01:33:35.000Z","updated":"2021-08-09T07:09:50.833Z","comments":true,"path":"2021/06/30/dcgan-03/","link":"","permalink":"https://gardenk11181.github.io/2021/06/30/dcgan-03/","excerpt":"","text":"Previous Article: DCGAN - (2) Architecture 이번 글에서는 DCGAN의 학습과정을 다뤄보자. 지금까지 그래왔듯이 pytorch구현을 위한 기술적인 부분은 미뤄두고, 개념만 다뤄볼 것이다. 기본적으로 Stochastic Gradient Descent(SGD)방법을 기반으로 하는 Adam Optimizer를 사용한다. SGD방법은 일반적인 Gradient Descent 방법과는 다르게 모든 데이터의 loss 값을 이용하지 않고 미리 정해둔 mini-batch 크기만큼 랜덤하게 추출한 데이터만을 가지고 진행하는 방법이다. 이렇게 확률적으로 진행되기 때문에 Stochastic이라는 명칭이 붙었다. Adam Optimizer에 대한 자세한 내용은 논문을 좀 더 공부한 후 적어보도록 하겠다. I. Initialization각 layer의 초기 weight들은 identically independent하게 을 따르는 값들로 설정하였고, LeakyReLU의 기울기는 0.2로 설정하였다. II. Train DiscriminatorDiscriminator는 주어진 image를 정확히 예측하는 것이 목표이다. 즉, 을 maximize하는 것이 목표이다. 리를 위하여 다음과 같은 과정을 따른다. mini-batch 크기만큼의 real 학습 데이터를 Discriminator에 넣어 loss 값을 구하고, backward로 Discriminator의 gradient를 구한다. mini-batch 크기만큼의 fake 데이터를 현재 Generator에서 생성하고, 이를 Discriminator에 넣어 loss 값을 구한다. 이후 마찬가지로 backward로 Discriminator의 gradient를 구한다. 1과 2에 의해 구해진 각 weight들에 대한 gradient의 평균을 가지고 Adam Optimizer를 통해 Discriminator의 parameter를 update한다. III. Train GeneratorGenerator는 진짜같은 fake image를 만드는 것이 목표이기에 를 minimize하는 방향으로 학습해야 한다. 그러나 논문에 따르면 이 방법이 학습 초기에 충분한 gradient를 제공하지 못한다고 한다. 따라서 pytorch에서는 이를 대신하여 를 maximize하는 방향으로 학습하였다. 이는 fake image를 fake로 인식하지 않는 방향으로 학습한다고 해석할 수 있기에 직관적으로 타당하다고 여겨진다. Discriminator 학습 시에 생성했던 fake images를 현재의 Discriminator에 넣어 loss 룰 구하고, backward로 Generator의 gradient를 구한다. 구한 gradient를 가지고 Adam Optimizer를 통해 Generator의 parameter를 update한다. 아래 그림은 pytorch에서 celeb-A데이터에 대하여 위와 같은 방법으로 학습시켰을 때의 Loss값의 변화를 나타낸 그래프이다. 최종적으로 1600여 데이터를 5번의 epoch로 학습시켰을 때, Real Image로 부터 얻어진 Fake Image는 다음과 같이 나온다고 한다. 이제 DCGAN구현을 위한 대략적인 공부는 마친 것 같다. 다음 글에서부터는 MNIST데이터를 가지고 DCGAN을 구현해보고 학습시켜보도록 하자. Next Article: DCGAN - (4) Implementation of Network for Generator","categories":[{"name":"AI","slug":"AI","permalink":"https://gardenk11181.github.io/categories/AI/"},{"name":"DCGAN","slug":"AI/DCGAN","permalink":"https://gardenk11181.github.io/categories/AI/DCGAN/"}],"tags":[{"name":"ai","slug":"ai","permalink":"https://gardenk11181.github.io/tags/ai/"},{"name":"deep learning","slug":"deep-learning","permalink":"https://gardenk11181.github.io/tags/deep-learning/"},{"name":"gan","slug":"gan","permalink":"https://gardenk11181.github.io/tags/gan/"}]},{"title":"DCGAN - (2) Architecture","slug":"dcgan-02","date":"2021-06-29T04:06:09.000Z","updated":"2021-08-09T07:08:03.467Z","comments":true,"path":"2021/06/29/dcgan-02/","link":"","permalink":"https://gardenk11181.github.io/2021/06/29/dcgan-02/","excerpt":"","text":"Previous Article: DCGAN - (1) Generative Adversarial Networks 이번 글에서는 DCGAN이 어떠한 Architecture을 가지는지 알아보자. 기본적으로는 Deep Neural Network의 한 종류인 CNN(Convolutional Neural Network)의 구조를 가진다. DCGAN의 CNN은 Convolutional layer와 Batch-norm layer, 그리고 ReLU로 이루어져 있는데 각 layer와 CNN에 대한 자세한 설명은 다음 기회에 해보도록 하고, 본 글에서는 이들이 DCGAN에서 어떻게 구성되어 있는지만 알아보도록 하자. Generator 위 그림과 같이 Generator는 latent vector를 image로 mapping 함에 있어 총 네 개의 convolutional layer로 이루어진 CNN 구조를 사용한다. 주어진 latent vector를 첫 번째 activation map으로 reshape 하는 layer까지 합하면 총 5개이다. 다만, Generator에서는 일반적인 convolutional layer가 아닌 convolutional transpose layer를 사용한다. 일반적인 convolutional layer는 큰 size의 input data를 작은 output data로 mapping 하지만, conv-transpose layer는 input data의 size보다 output data의 size가 더 크다는 근본적인 차이점이 있다. 자세한 내용은 역시 CNN 관련 글에서 알아보도록 하자. DiscriminatorDiscriminator는 Generator의 역순이라고 생각하면 좋다. 즉, 주어진 image를 CNN을 통해 scala probability로 mapping 한다. 마찬가지로 총 네 개의 convolutional layer가 사용되는데, Discriminator는 점차 차원을 줄여나가기에 일반적인 convolutional layer를 사용한다. 또한, 일반적인 ReLU function을 사용하지 않고 위 그림과 같이 에서의 기울기가 0이 아닌 작은 양의 값을 가지는 LeakyReLU function을 사용하였다. 지금까지 아주 간단하게 DCGAN의 Architecture를 알아보았고, 다음 글에서는 이 모델을 어떻게 학습시키는지 알아보자. Next Article: DCGAN - (3) Training","categories":[{"name":"AI","slug":"AI","permalink":"https://gardenk11181.github.io/categories/AI/"},{"name":"DCGAN","slug":"AI/DCGAN","permalink":"https://gardenk11181.github.io/categories/AI/DCGAN/"}],"tags":[{"name":"ai","slug":"ai","permalink":"https://gardenk11181.github.io/tags/ai/"},{"name":"deep learning","slug":"deep-learning","permalink":"https://gardenk11181.github.io/tags/deep-learning/"},{"name":"gan","slug":"gan","permalink":"https://gardenk11181.github.io/tags/gan/"}]},{"title":"DCGAN - (1) Generative Adversarial Networks","slug":"dcgan-01","date":"2021-06-27T03:46:33.000Z","updated":"2021-08-09T07:05:32.620Z","comments":true,"path":"2021/06/27/dcgan-01/","link":"","permalink":"https://gardenk11181.github.io/2021/06/27/dcgan-01/","excerpt":"","text":"인턴 1주 차 목표는 pytorch가 제공하는 DCGAN tutorial을 공부해오는 것이었다. 디테일한 코드보다는 용어의 의미와 전체적인 흐름 및 이론에 주목하여 살펴보도록 하자. GAN (Generative Adversarial Networks)먼저, GAN 이란 Generator와 Discriminator로 이루어진 Adversarial Networks로, 주어진 data의 분포와 비슷한 분포의 sample을 추출하고자 하는 generative model이다. 여기서 ‘Adversarial(적대적)’이라 표현한 이유는, 후술하겠지만 Generator와 Discriminator가 서로 mini-max 게임을 하는 관계이기 때문이다. Generator는 latent vector를 가지고 fake image를 만들고, Discriminator는 주어진 image가 진짜인지 아닌지를 판별한다. 즉, Discriminator는 주어진 image가 진짜인지 아닌지 정확하게 분류하고자 하는 것이 목표이고, Generator는 이러한 Discriminator가 자기가 만든 fake image를 real image로 인식하게끔 하는 것이 목표이다. 이러한 이유로 Adversarial Networks라고 표현하였다. Loss Function앞서 말한 GAN의 의미를 이해한다면, 다음과 같이 정의된 Loss Function은 꽤나 자연스럽게 다가온다. Discriminator는 training data\b x를 real로, Generator에 의해 생성된 G(z)를 fake로 분류할 확률을 maximize 한다. 이와 동시에 Generator는 Discriminator가 G(z)를 fake로 분류할 확률을 minimize 한다. GAN을 제시한 논문에 따르면 두 분포 가 같을 때가 본 mini-max game의 solution이지만, 수렴조건에 관한 부분은 아직 연구 중에 있다고 한다. DCGAN은 이러한 GAN을 Deep Convolutional Architecture를 가지고 구현한 모델에 해당한다. 자세한 내용은 다음 글에서 다뤄보자. Next Article: DCGAN - (2) Architecture","categories":[{"name":"AI","slug":"AI","permalink":"https://gardenk11181.github.io/categories/AI/"},{"name":"DCGAN","slug":"AI/DCGAN","permalink":"https://gardenk11181.github.io/categories/AI/DCGAN/"}],"tags":[{"name":"ai","slug":"ai","permalink":"https://gardenk11181.github.io/tags/ai/"},{"name":"deep learning","slug":"deep-learning","permalink":"https://gardenk11181.github.io/tags/deep-learning/"},{"name":"gan","slug":"gan","permalink":"https://gardenk11181.github.io/tags/gan/"}]}],"categories":[{"name":"AI","slug":"AI","permalink":"https://gardenk11181.github.io/categories/AI/"},{"name":"VAE","slug":"AI/VAE","permalink":"https://gardenk11181.github.io/categories/AI/VAE/"},{"name":"DCGAN","slug":"AI/DCGAN","permalink":"https://gardenk11181.github.io/categories/AI/DCGAN/"}],"tags":[{"name":"ai","slug":"ai","permalink":"https://gardenk11181.github.io/tags/ai/"},{"name":"deep learning","slug":"deep-learning","permalink":"https://gardenk11181.github.io/tags/deep-learning/"},{"name":"vae","slug":"vae","permalink":"https://gardenk11181.github.io/tags/vae/"},{"name":"gan","slug":"gan","permalink":"https://gardenk11181.github.io/tags/gan/"}]}